{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9293afee",
   "metadata": {},
   "source": [
    "# ğŸ“š AIë¥¼ ì´ìš©í•œ ìš©ìˆ˜ìˆ˜ìš” ì¶”ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562aecc6",
   "metadata": {},
   "source": [
    "## 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì…ë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a43d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from math import sqrt\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd  # Basic library for all of our dataset operations\n",
    "import pmdarima as pm\n",
    "import shap\n",
    "import statsmodels as sm\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from prophet import Prophet\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from matplotlib import pyplot as plt\n",
    "from pylab import rcParams\n",
    "from sklearn import linear_model, svm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa import api as smt\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from statsmodels.tsa.arima_model import ARIMA, ARMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from tqdm import tqdm\n",
    "\n",
    "from metrics import evaluate\n",
    "from plots import bar_metrics\n",
    "\n",
    "# We will use deprecated models of statmodels which throw a lot of warnings to use more modern ones\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Extra settings\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "plt.style.use('bmh')\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['text.color'] = 'k'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rc('font', family=\"NanumGothic\")\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bafa61",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (CASE_STUDY: ì„œìš¸íŠ¹ë³„ì‹œ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718640fb-1340-4d56-a9ed-4c2b825c7b15",
   "metadata": {},
   "source": [
    "### 2.1 ê³¼ê±° ì˜í–¥ì¸ì ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    " * 2001~2021ë…„ ì›”ë³„ ì¸êµ¬ê´€ë ¨ ì˜í–¥ì¸ì 7ê°œ (\"ì´ì¸êµ¬ìˆ˜\", 'ì„¸ëŒ€ìˆ˜', 'ì„¸ëŒ€ë‹¹ ì¸êµ¬', 'ë‚¨ì ì¸êµ¬ìˆ˜', 'ì—¬ì ì¸êµ¬ìˆ˜', 'ë‚¨ì—¬ ë¹„ìœ¨', 'ê³ ë ¹í™”ë¹„ìœ¨')\n",
    "               ì›”ë³„ ê¸°ìƒê´€ë ¨ ì˜í–¥ì¸ì 3ê°œ (\"ê¸°ì˜¨\", \"ì›”ê°•ìˆ˜ëŸ‰\", \"ìŠµë„\") ì—‘ì…€ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc918f63-402c-4239-9dd3-0bfe4574c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œêµ°ëª… ì§€ì •\n",
    "CITY_NAME = 'ì„œìš¸íŠ¹ë³„ì‹œ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b8fe9-a08e-4c45-a610-16770362dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_1 = pd.read_excel(\"../data/ì˜í–¥ë¶„ì„/ì‹œêµ°ë³„_ì˜í–¥ì¸ì(ë…ë¦½ë³€ìˆ˜)/íŠ¹ê´‘ì—­ì‹œ/íŠ¹ë³„ì‹œ ë° ê´‘ì—­ì‹œ ì¢…í•©.xlsx\", sheet_name=CITY_NAME) # ì‹œêµ°ë³„ ì˜í–¥ì¸ì ì—‘ì…€íŒŒì¼ ì½ê¸°\n",
    "factor_1_1 = factor_1.iloc[3:, :29] # header ë° ì¼ë‹¨ìœ„ìë£Œ ì‚­ì œ\n",
    "factor_1_1.columns = factor_1.iloc[2, :29] # set column names\n",
    "data_len = factor_1_1['ì—°ë„'].isna().argmax()\n",
    "factor_1_1 = factor_1_1[:data_len]\n",
    "factor_1_1.index = pd.to_datetime(factor_1_1['ì—°ë„'][:data_len].astype(str) + '-' + factor_1_1['ì›”'][:data_len].astype(str))\n",
    "factor_1_1.rename_axis(columns='', inplace=True)\n",
    "factor_1_1.drop(columns=['ì—°ë„', 'ì›”'], inplace=True)\n",
    "# Simplify Column names\n",
    "factor_1_1.columns = ['ì´ì¸êµ¬ìˆ˜', 'ì„¸ëŒ€ìˆ˜', 'ì„¸ëŒ€ë‹¹ ì¸êµ¬', 'ë‚¨ì ì¸êµ¬ìˆ˜', 'ì—¬ì ì¸êµ¬ìˆ˜', 'ë‚¨ì—¬ ë¹„ìœ¨', 'ê³ ë ¹í™”ë¹„ìœ¨', 'ì „ë ¥ì‚¬ìš©ëŸ‰', 'ì‚°ì—…ì²´ ìˆ˜', '100ì¸ ì´ìƒ ì‚¬ì—…ì¥', \n",
    "                       'ì‚°ì—…ë‹¨ì§€ë©´ì ', 'ì—°ê°„ ê°€ê³„ì†Œë“', 'ì—°ê°„ ê³ ë“±í•™êµ ì¡¸ì—…(ëª…)', 'ì—°ê°„ ê³ ë“±í•™êµ ì¡¸ì—…ì ë¹„ìœ¨', 'ì—°ê°„ ì¸ê±´ë¹„', 'ì—°ê°„ í‰ê·  ìˆ˜ì…ëŒ€ë¹„ ì¸ê±´ë¹„', 'ì·¨ì—…ë¥ (ëª…)', \n",
    "                       'ê³ ìš©ë³´í—˜ì ë¹„ìœ¨', 'ì·¨ì•½ê³„ì¸µ(ê°œ)', 'ì·¨ì•½ê³„ì¸µ(ë¹„ìœ¨)', 'ê¸°ì˜¨', 'ì›”ê°•ìˆ˜ëŸ‰', 'ìŠµë„', 'ì¼ì¡°ëŸ‰', 'ì§€ì¤‘ì˜¨ë„', 'í’ëŸ‰', 'ê¸°ì••']\n",
    "# Change datatype from object to float\n",
    "factor_1_1 = factor_1_1.astype(float)\n",
    "# í•´ë‹¹ ì»¬ëŸ¼ë§Œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "factors = factor_1_1[[\"ì´ì¸êµ¬ìˆ˜\", 'ì„¸ëŒ€ìˆ˜', 'ì„¸ëŒ€ë‹¹ ì¸êµ¬', 'ë‚¨ì ì¸êµ¬ìˆ˜', 'ì—¬ì ì¸êµ¬ìˆ˜', 'ë‚¨ì—¬ ë¹„ìœ¨', 'ê³ ë ¹í™”ë¹„ìœ¨', \"ê¸°ì˜¨\", \"ì›”ê°•ìˆ˜ëŸ‰\", \"ìŠµë„\"]]\n",
    "factors.index.name = \"date\"\n",
    "factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aede48-3380-40cb-94ce-098ffdc38d82",
   "metadata": {},
   "source": [
    "### 2.2 ê³¼ê±° ì¢…ì†ë³€ìˆ˜ ë°ì´í„° (ìš©ìˆ˜ê³µê¸‰ëŸ‰) ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    " * 2001~2021ë…„ ì›”ë³„ ìš©ìˆ˜ê³µê¸‰ëŸ‰ ë°ì´í„° ì—‘ì…€ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc7844-b7a5-4495-a642-97d5be0b464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_supply = pd.read_excel(\"../data/ì˜í–¥ë¶„ì„/ì‹œêµ°ë³„_ìš©ìˆ˜ê³µê¸‰ëŸ‰(ì¢…ì†ë³€ìˆ˜).xlsx\", sheet_name=\"ì¢…í•©(ë„ë³„ ì •ë ¬)\", header=1)\n",
    "water_supply.set_index(\"ì§€ìì²´ëª…\", inplace=True)\n",
    "water_supply = water_supply.iloc[1:, 1:2]\n",
    "water_supply.rename(columns = {CITY_NAME:f'{CITY_NAME}_ë¬¼ê³µê¸‰ëŸ‰'}, inplace = True)\n",
    "water_supply = water_supply.astype(float)\n",
    "water_supply.index = pd.to_datetime(water_supply.index.astype(str))\n",
    "water_supply.index.name = \"date\"\n",
    "water_supply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fec5362-f929-4552-9ca8-cb69ddafdae8",
   "metadata": {},
   "source": [
    "### 2.3 ë¯¸ë˜ ì˜í–¥ì¸ì ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    " * 2022~2037ë…„ ë…„ë³„ \"ì´ì¸êµ¬ìˆ˜\", 'ì„¸ëŒ€ìˆ˜', 'ì„¸ëŒ€ë‹¹ ì¸êµ¬', 'ë‚¨ì ì¸êµ¬ìˆ˜', 'ì—¬ì ì¸êµ¬ìˆ˜', 'ë‚¨ì—¬ ë¹„ìœ¨', 'ê³ ë ¹í™”ë¹„ìœ¨' ìë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7928deb-1d00-40bd-9fbe-02b931cd8cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_factor = pd.read_excel(\"../data/ìˆ˜ìš”ì˜ˆì¸¡/ì‹œêµ°ë³„_ì˜í–¥ì¸ì_ì¥ë˜ì˜ˆì¸¡/íŠ¹ê´‘ì—­ì‹œ.xlsx\", sheet_name=CITY_NAME)\n",
    "future_factor_1 = future_factor.iloc[3:19, :8] # header ë° ì¼ë‹¨ìœ„ìë£Œ ì‚­ì œ\n",
    "future_factor_1.columns = future_factor.iloc[2, :8] # set column names\n",
    "future_factor_1.index = pd.to_datetime(future_factor_1[\"ì—°ë„\"], format='%Y')\n",
    "future_factor_1 = future_factor_1.resample('MS').ffill()\n",
    "future_factor_1.index.name = \"date\"\n",
    "future_factors1 = future_factor_1[future_factor_1.columns[1:]]\n",
    "future_factors1.columns = ['ì´ì¸êµ¬ìˆ˜', 'ì„¸ëŒ€ìˆ˜', 'ì„¸ëŒ€ë‹¹ ì¸êµ¬', 'ë‚¨ì ì¸êµ¬ìˆ˜', 'ì—¬ì ì¸êµ¬ìˆ˜', 'ë‚¨ì—¬ ë¹„ìœ¨', 'ê³ ë ¹í™”ë¹„ìœ¨']\n",
    "future_factors1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade5d23-5773-40ca-87de-b181bebd125c",
   "metadata": {},
   "source": [
    "### 2.4 ë¯¸ë˜ ê¸°í›„ ì˜í–¥ì¸ì ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    " * 2022~2040ë…„ ë…„ë³„ \"ì›”ê°•ìˆ˜ëŸ‰\", \"ìŠµë„\", \"ìµœê³ ì˜¨ë„\", \"ìµœì €ì˜¨ë„\", \"í‰ê· ì˜¨ë„\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f505d-2022-447f-8d63-6d6a2bd1a992",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_factor_3 = future_factor.iloc[3:, 22:28] # header ë° ì¼ë‹¨ìœ„ìë£Œ ì‚­ì œ\n",
    "future_factor_3.columns = future_factor.iloc[2, 22:28] # set column names\n",
    "future_factor_3.index = pd.to_datetime(future_factor_3[\"ë‚ ì§œ\"], format='%Y%M%D')\n",
    "future_factor_3.index.name = \"date\"\n",
    "future_factors2 = future_factor_3[['ì›”í•©ê°•ìˆ˜ëŸ‰(mm)', 'ìŠµë„\\ní‰ê· ìƒëŒ€ìŠµë„(%)', 'í‰ê· ì˜¨ë„(Â°C)']]\n",
    "future_factors2.columns = ['ì›”ê°•ìˆ˜ëŸ‰', 'ìŠµë„', 'ê¸°ì˜¨']\n",
    "future_factors2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62db96d-ed96-40c2-ba91-d0f96885a7dc",
   "metadata": {},
   "source": [
    "### 2.5 ìš©ìˆ˜ìˆ˜ìš” ì¶”ì •ì„ ìœ„í•œ ê³¼ê±° ë° ë¯¸ë˜ìë£Œ í•©ì¹˜ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9068516-5064-4514-a44e-281bdd164f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.merge(water_supply, factors, on=\"date\", how=\"left\")\n",
    "future_factors1_1 = future_factors1.loc[\"2022-01-01\":\"2035-01-01\"]\n",
    "future_factors2_1 = future_factors2.loc[\"2022-01-01\":\"2035-01-01\"]\n",
    "future_factor = pd.concat([future_factors1_1, future_factors2_1], axis=1)\n",
    "total_data_df = pd.concat([total_data, future_factor], axis=0)\n",
    "total_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea110bd-ad0a-4612-bcca-2b9c82b0a3e9",
   "metadata": {},
   "source": [
    "### 2.6 ì´ìš©ê°€ëŠ¥í•œ ë°ì´í„° ê¸°ê°„ ê²€í† \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d31c3f-3e62-41d7-9a43-a7571ea39846",
   "metadata": {},
   "source": [
    " - ë¬¼ê³µê¸‰ëŸ‰: 1991-01ì›” ~ 2021-12ì›”\n",
    " - ì§€ì—­ì„± ì˜í–¥ì¸ì: 2008-01ì›” ~ 2034-12ì›”\n",
    " - ê³„ì ˆì„± ì˜í–¥ì¸ì: 2001-01ì›” ~ 2034-12ì›”  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd568b8-24ea-45ce-8f48-0ff0bac64c83",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> ì§€ì—­ì„± ì˜í–¥ì¸ìì™€ ê³„ì ˆì„± ì˜í–¥ì¸ìë¥¼ í™œìš©í•œ ìš©ìˆ˜ìˆ˜ìš” ì¶”ì • ë°ì´í„° í˜„í™©\n",
    "    \n",
    "<b>   - ë°ì´í„° ì´ìš©ê°€ëŠ¥ ê¸°ê°„: 2008-01ì›”~2021ë…„12ì›” (ì´ 168ê°œ ë°ì´í„°) <b>\n",
    "    \n",
    "<b>   - Training       ê¸°ê°„: 2008-01ì›”~2021ë…„12ì›” (ì´ 168ê°œ ë°ì´í„°)  <b>\n",
    "    \n",
    "<b>   - Testing        ê¸°ê°„: 2008-01ì›”~2021ë…„12ì›” (ì´ 168ê°œ ë°ì´í„°) </b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b932fde8-5e90-4255-8b56-0f4f69dc2b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "sns.heatmap(total_data_df.isnull(), cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc5f23-818e-4924-b4d3-a86e206ee21b",
   "metadata": {},
   "source": [
    "## 3. ì‹œê³„ì—´ ìë£Œ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e4fd4a-a52b-47dc-a958-18195719a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_df_ts_analysis = total_data_df.loc[\"2008-01-01\":\"2021-12-01\"]\n",
    "total_data_df_ts_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041db01-b999-4dcd-9083-66e7ad5414f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_df_ts_analysis[\"ì„œìš¸íŠ¹ë³„ì‹œ_ë¬¼ê³µê¸‰ëŸ‰\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db2f929",
   "metadata": {},
   "source": [
    "Lets check each feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c15dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 18, 17\n",
    "values = total_data_df_ts_analysis.values\n",
    "groups = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "i = 1\n",
    "# plot each column\n",
    "for group in groups:\n",
    "    plt.subplot(len(groups), 1, i)\n",
    "    plt.plot(values[:, group])\n",
    "    plt.title(total_data_df_ts_analysis.columns[group], y=0.5, loc='right')\n",
    "    i += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(30, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.title('Water Demand (%s)'%CITY_NAME, fontsize=30)\n",
    "\n",
    "plt.plot(total_data_df_ts_analysis[\"ì„œìš¸íŠ¹ë³„ì‹œ_ë¬¼ê³µê¸‰ëŸ‰\"])\n",
    "plt.savefig(\"results/water_demand.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55520fb1",
   "metadata": {},
   "source": [
    "# ğŸ“š Time series analysis and transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c527a3",
   "metadata": {},
   "source": [
    "This notebook contains a set of operations we can perform in our time series in order to get some insights or transform the series to make forecasting easier.\n",
    "\n",
    "Which ones will we touching in this notebook?\n",
    "\n",
    "* Time series decomposition\n",
    "  * Level\n",
    "  * Trend\n",
    "  * Seasonality\n",
    "  * Noise\n",
    "\n",
    "* Stationarity\n",
    "  * AC and PAC plots\n",
    "  * Rolling mean and std\n",
    "  * Dickey-Fuller test\n",
    "\n",
    "* Making our time series stationary\n",
    "  * Difference transform\n",
    "  * Log scale\n",
    "  * Smoothing\n",
    "  * Moving average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6899a75",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "## Decomposing our time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e65ad",
   "metadata": {},
   "source": [
    "One of the most common analysis for time series is decomposing it into multiple parts. The parts we can divide a time series into are: level, trend, seasonality and noise, all series contain level and noise but seasonality and trend are not always present (there will be more analysis for this two parts).\n",
    "\n",
    "This 4 parts can combine either additively or multiplicatively into the time series.\n",
    "\n",
    "##### Additive Model\n",
    "`y(t) = Level + Trend + Seasonality + Noise`\n",
    "\n",
    "Additives models are lineal. Trend is linear and seasonality has constant frequency and amplitude. Change is constant over time\n",
    "\n",
    "##### Multiplicative model\n",
    "`y(t) = Level * Trend * Seasonality * Noise`\n",
    "\n",
    "Multiplicatives models are nonlinear,trend is curved and seasonality is not constant. Change is not constant over time\n",
    "\n",
    "Decomposing is used to analyse the time series. Identify each one of the different parts of the time series and its behaviour, each of the components may affect your models in different ways.\n",
    "\n",
    "Most time series are a combination of a additive model and a multiplicate model, is hard to identify real world time series into one single model.\n",
    "\n",
    "##### Automatic time series decomposition\n",
    "\n",
    "Statsmodel python library provides a function [seasonal_compose()](http://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html) to automatically decompose a time series, you still need to specify wether the model is additive or multiplicative. We will use multiplicative as our quick peak at the pm2.5 time series shows no linear trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2b19b2-8e69-4505-a636-f3eb1f842393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba028752-fdfd-4284-94b4-518dd53228ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 18, 8\n",
    "plt.figure(num=None, figsize=(50, 20), dpi=80, facecolor='w', edgecolor='k')\n",
    "series = total_data_df_ts_analysis[\"ì„œìš¸íŠ¹ë³„ì‹œ_ë¬¼ê³µê¸‰ëŸ‰\"][:168]\n",
    "result = seasonal_decompose(series, model='additive')\n",
    "result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 18, 8\n",
    "plt.figure(num=None, figsize=(50, 20), dpi=80, facecolor='w', edgecolor='k')\n",
    "series = total_data_df_ts_analysis[\"ì„œìš¸íŠ¹ë³„ì‹œ_ë¬¼ê³µê¸‰ëŸ‰\"][:168]\n",
    "result = seasonal_decompose(series, model='multiplicative')\n",
    "result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6895620",
   "metadata": {},
   "source": [
    "### Level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc9448",
   "metadata": {},
   "source": [
    "Level simply means the current value of the series once we remove trend, seasonality and the random noise. This are the true values that come from the series itself and we will try to predict with our models. Most of the models will benefit the more our time series is composed by the level and not trends/seasonality/noise. We also present models capable of handling seasonality and trend (non stationary series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d63ec",
   "metadata": {},
   "source": [
    "### Trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c56c73",
   "metadata": {},
   "source": [
    "A trend is observed when there is an increasing or decreasing slope observed in the time series. A trend is a smooth, general, long-term, average tendency. It is not always necessary that the increase or decrease is in the same direction throughout the given period of time.\n",
    "\n",
    "Trend can be removed from your time series data (and data in the future) as a data preparation and cleaning exercise. This is common when using statistical methods for time series forecasting, but does not always improve results when using machine learning models. We will see different methods for this in the making your series stationary section\n",
    "\n",
    "In practice, identifying a trend in a time series can be a subjective process as we are never sure if contains seasonalities or noise to it,\n",
    "Create line plots of your data and inspect the plots for obvious trends.\n",
    "\n",
    "Now we will try some methods to check for trend in our series:\n",
    "* Automatic decomposing\n",
    "* Moving average\n",
    "* Fit a linear regression model to identify trend\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "layout = (3, 2)\n",
    "pm_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "mv_ax = plt.subplot2grid(layout, (1, 0), colspan=2)\n",
    "fit_ax = plt.subplot2grid(layout, (2, 0), colspan=2)\n",
    "\n",
    "pm_ax.plot(result.trend)\n",
    "pm_ax.set_title(\"Automatic decomposed trend\")\n",
    "\n",
    "mm = water_predict.water_demand[:168].rolling(3).mean()\n",
    "mv_ax.plot(mm)\n",
    "mv_ax.set_title(\"Moving average 12 steps\")\n",
    "\n",
    "\n",
    "X = [i for i in range(0, len(water_predict.water_demand[:168]))]\n",
    "X = np.reshape(X, (len(X), 1))\n",
    "y = water_predict.water_demand[:168].values\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "# calculate trend\n",
    "trend = model.predict(X)\n",
    "fit_ax.plot(trend)\n",
    "fit_ax.set_title(\"Trend fitted by linear regression\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b9904",
   "metadata": {},
   "source": [
    "We can see our series does not have a strong trend, results from both the automatic decomposition and the moving average look more like a seasonality efect+random noise than a trend. This sort of confirmed with our linear regression, which cant find our series properly and gives us a poor trend.\n",
    "\n",
    "We could also try to split our series into smaller ones to try identify subtrends with the mentioned methods but we will not be doing so in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f84b0",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "### Seasonality\n",
    "Seasonality is observed when there is a distinct repeated pattern observed between regular intervals due to seasonal factors. It could be because of the month of the year, the day of the month, weekdays or even time of the day. For example the amount of sunscream protector (always low in winter and high in summer).\n",
    "\n",
    "The automatic decomposing chart did not gave us a good look into the decomposed seasonality, let's try decomposing smaller parts of the time series first and test seasonalities we found into the others.\n",
    "\n",
    "Lets go with the first year of data only now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c71545",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 18, 8\n",
    "plt.figure(num=None, figsize=(50, 20), dpi=80, facecolor='w', edgecolor='k')\n",
    "series = water_predict.water_demand[:168]\n",
    "result = seasonal_decompose(series, model='multiplicative')\n",
    "result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf22d65",
   "metadata": {},
   "source": [
    "Here can see a clear weekly trend, 4 spikes every month (weerkly). Lets check how the last year of data looks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211a5db0",
   "metadata": {},
   "source": [
    "##INTERPRETATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed44a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for weekly seasonality\n",
    "resample = water_predict[:168].resample('Y')\n",
    "weekly_mean = resample.mean()\n",
    "weekly_mean.water_demand.plot(label='Yearly mean')\n",
    "plt.title(\"Resampled series to weekly mean values\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee62d25",
   "metadata": {},
   "source": [
    "**Manual methods to find seasonalities**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e37b47",
   "metadata": {},
   "source": [
    "We can also try to generate a model to find the seasonalities for us. One of the most common to use is a simple polynomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e61eff-69bf-41a0-99d7-b25d62035055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix xticks to show dates\n",
    "# fit polynomial: x^2*b1 + x*b2 + ... + bn\n",
    "series = water_predict.water_demand[:168].values\n",
    "X = [i % 365 for i in range(0, len(series))]\n",
    "y = series\n",
    "degree = 100\n",
    "coef = np.polyfit(X, y, degree)\n",
    "# create curve\n",
    "curve = list()\n",
    "for i in range(len(X)):\n",
    "    value = coef[-1]\n",
    "    for d in range(degree):\n",
    "        value += X[i]**(degree-d) * coef[d]\n",
    "    curve.append(value)\n",
    "# plot curve over original data\n",
    "plt.plot(series, label='Original')\n",
    "plt.plot(curve, color='red', linewidth=3, label='polynomial model')\n",
    "plt.legend()\n",
    "plt.title(\"Polynomial fit to find seasonality\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac358c43",
   "metadata": {},
   "source": [
    "We can see how the model to find a seasonality fits poorly to our data. Is going to be a complicate time series to model :P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901fdea9",
   "metadata": {},
   "source": [
    "### Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e538cd",
   "metadata": {},
   "source": [
    "Our time series will also have a noise component to them, most likely [white noise](https://en.wikipedia.org/wiki/White_noise). We say white noise is present if the measurement are independent and identically distributed with a mean of zero. This will mean all our measurements have same variance and no correlation with the rest of values in the series.\n",
    "\n",
    "If our time series has white noise this will mean we can't predict that component of the series (as is random) and we shoul aim to produce a model with errors close to this white noise.\n",
    "\n",
    "How to check if our series has white noise?\n",
    "* Check our series histogram, does it look like a Gaussian distribution? Mean=0 and constand std\n",
    "* Correlation plots\n",
    "* Standard deviation distribution, is it a Gaussian distribution?\n",
    "* Does the mean or level change over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eced56ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 7))\n",
    "layout = (2, 2)\n",
    "hist_ax = plt.subplot2grid(layout, (0, 0))\n",
    "ac_ax = plt.subplot2grid(layout, (1, 0))\n",
    "hist_std_ax = plt.subplot2grid(layout, (0, 1))\n",
    "mean_ax = plt.subplot2grid(layout, (1, 1))\n",
    "\n",
    "water_predict.water_demand.hist(ax=hist_ax)\n",
    "hist_ax.set_title(\"Original series histogram\")\n",
    "\n",
    "plot_acf(series, lags=30, ax=ac_ax)\n",
    "ac_ax.set_title(\"Autocorrelation\")\n",
    "\n",
    "mm = water_predict.water_demand.rolling(3).std()\n",
    "mm.hist(ax=hist_std_ax)\n",
    "hist_std_ax.set_title(\"Standard deviation histogram\")\n",
    "\n",
    "mm = water_predict.water_demand[:168].rolling(12).mean()\n",
    "mm.plot(ax=mean_ax)\n",
    "mean_ax.set_title(\"Mean over time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5416f2",
   "metadata": {},
   "source": [
    "We can see our series do not follow a Gaussian distribution from the histogram and neither the standard deviation, thought the std does has the mean more centered which shows a small part of white noise that is not possible to split from the original series (this will happen most of the times, specially is real life datasets)).\n",
    "\n",
    "We also have a small correlation with close measurements in time but not present with distant measurements (this could also indicate low seasonality). The mean over time also shows something similar with a constant value and high peaks in the same moments for the 4 years (smaller in 2012)\n",
    "\n",
    "We could say our series does contain a small part of white noise but it is really small and hard to remove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc1f64",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "## Stationarity\n",
    "Stationarity is an important characteristic of time series. A time series is stationarity if it has constant mean and variance over time. Most models work only with stationary data as this makes it easier to model. Not all time series are stationary but we can transform them into stationary series in different ways.\n",
    "\n",
    "Often, stock prices are not a stationary process, since we might see a growing trend, or its volatility might increase over time (meaning that variance is changing).\n",
    "\n",
    "### Check for sationarity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562a5ad",
   "metadata": {},
   "source": [
    "### Autocorrelation and Partial autocorrelation plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee7e07b",
   "metadata": {},
   "source": [
    "Autocorelation plots show how correlated are values at time t with the next values in time t+1,t+2,..t+n. If the data would be non-stationary the autocorrelation values will be highly correlated with distant points in time showing possible seasonalities or trends.\n",
    "\n",
    "Stationary series autocorrelation values will quickly decrease over time t. This shows us that no information is carried over time and then the series should be constant over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b7a556",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(series, lags=12)\n",
    "plot_pacf(series, lags=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88baa7b0",
   "metadata": {},
   "source": [
    "We saw that our time series values are not correlated with distant points in time, this is good and shows us our series should be stationary but for the shake of learning and confirming we will test with some other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b31930",
   "metadata": {},
   "source": [
    "### Rolling means and standard deviation of our series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d0b2b",
   "metadata": {},
   "source": [
    "We were talking about how our mean and standard deviation should be constant over time in order to have a stationary time series, why not just plot this two properties?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e37c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determing rolling statistics\n",
    "rolmean = water_predict.water_demand.rolling(window=12).mean()\n",
    "rolstd = water_predict.water_demand.rolling(window=12).std()\n",
    "\n",
    "# Plot rolling statistics:\n",
    "orig = plt.plot(water_predict.water_demand, label='Original')\n",
    "mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "std = plt.plot(rolstd, color='black', label='Rolling Std')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Rolling Mean & Standard Deviation')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988eb21",
   "metadata": {},
   "source": [
    "We can see how our mean and standar deviation have a constant behaviour over the years, even if they change over the year this behaviour is then repeated next year. This proves us again a stationary series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76379ad",
   "metadata": {},
   "source": [
    "### Augmented Dickey-Fuller test\n",
    "The Augmented Dickey-Fuller test is a type of statistical test called a unit root test. The intuition behind a unit root test is that it determines how strongly a time series is defined by a trend. There are a number of unit root tests and the Augmented Dickey-Fuller may be one of the more widely used. It uses an autoregressive model and optimizes an information criterion across multiple different lag values.\n",
    "\n",
    "The null hypothesis of the test is that the time series can be represented by a unit root, that it is not stationary (has some time-dependent structure). The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary.\n",
    "\n",
    "Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n",
    "Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n",
    "We interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).\n",
    "\n",
    "p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n",
    "p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.\n",
    "Below is an example of calculating the Augmented Dickey-Fuller test on the Daily Female Births dataset. The statsmodels library provides the adfuller() function that implements the test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4422f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = water_predict.water_demand[:168].values\n",
    "result = adfuller(X)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3f8c0a",
   "metadata": {},
   "source": [
    "Here we also provide a method to quickly perform all the previous methods into one single function call and a pretty graph :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f21e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplot(y, lags=None, figsize=(12, 7), syle='bmh'):\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "\n",
    "    with plt.style.context(style='bmh'):\n",
    "        fig = plt.figure(figsize=(12, 7))\n",
    "        layout = (3, 2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "        mean_std_ax = plt.subplot2grid(layout, (2, 0), colspan=2)\n",
    "        y.plot(ax=ts_ax)\n",
    "        p_value = sm.tsa.stattools.adfuller(y)[1]\n",
    "        hypothesis_result = \"We reject stationarity\" if p_value <= 0.05 else \"We can not reject stationarity\"\n",
    "        ts_ax.set_title(\n",
    "            'Time Series stationary analysis Plots\\n Dickey-Fuller: p={0:.5f} Result: {1}'.format(p_value, hypothesis_result))\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        rolmean = water_predict.water_demand[:168].rolling(window=12).mean()\n",
    "        rolstd = water_predict.water_demand[:168].rolling(window=12).std()\n",
    "\n",
    "        # Plot rolling statistics:\n",
    "        orig = plt.plot(water_predict.water_demand[:168], label='Original')\n",
    "        mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "        std = plt.plot(rolstd, color='black', label='Rolling Std')\n",
    "        plt.legend(loc='best')\n",
    "        plt.title('Rolling Mean & Standard Deviation')\n",
    "\n",
    "\n",
    "tsplot(water_predict.water_demand[:168], lags=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db2f61",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "## Making Time Series Stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e6e1f",
   "metadata": {},
   "source": [
    "Okay we got lucky with this dataset and is already stationary, but what happens when this is not the case? We included a dummy dataset called `international_airline_passengers.csv` on the datasets folders which is not stationary and we will apply some methods in this section to transform it into a stationary series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_predict1 = pd.read_csv('datasets/seoul.csv')\n",
    "water_predict1.water_demand[:168].plot(label='Original')\n",
    "water_predict1.water_demand[:168].rolling(window=12).mean().plot(\n",
    "    color='red', label='Windowed mean')\n",
    "water_predict1.water_demand[:168].rolling(window=12).std().plot(\n",
    "    color='black', label='Std mean')\n",
    "plt.legend()\n",
    "plt.title('Original vs Windowed mean vs Windowed std')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62e2238",
   "metadata": {},
   "source": [
    "Lets run our stationary multitest function over this series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6375a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(water_predict1.water_demand[:168], lags=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b55137b",
   "metadata": {},
   "source": [
    "With a p value of ~1 and high correlation values over time distant samples (showing a clear seasonality shape) we need to apply some methods to make the series stationary.\n",
    "\n",
    "Coming back to the stationary definition, what makes our current series non stationary?\n",
    "\n",
    "**Trend** - The mean for our series is not constant, it increases over time and\n",
    "\n",
    "**Seasonality** - The values of our series vary over time with an specific pattern that repeats over time, this is called seasonalities (spike of people flying on the 24th of December)\n",
    "\n",
    "We now present some methods to remove or smotth this trend and seasonality components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b155da90",
   "metadata": {},
   "source": [
    "### Difference transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7579bd2",
   "metadata": {},
   "source": [
    "Applying a difference transform to a time series could help remove the series dependence on time.\n",
    "\n",
    "This transform is done by substracting the previous obesvation to the current one.\n",
    "\n",
    "`difference(t) = observation(t) - observation(t-1)`\n",
    "\n",
    "Taking the difference between consecutive observations would be a lag-1 difference, we can tweek this lag value to fit our series.\n",
    "\n",
    "We can also apply differencing transforms consecutively in the same series if the temporal effect hasnt been removed yet. This is called multiple order difference transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(dataset, interval=1, order=1):\n",
    "    for u in range(order):\n",
    "        diff = list()\n",
    "        for i in range(interval, len(dataset)):\n",
    "            value = dataset[i] - dataset[i - interval]\n",
    "            diff.append(value)\n",
    "        dataset = diff\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e02d5a-48b9-4439-b64d-0e17249c00c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag1series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01decccf-f5db-49f0-8688-ecab3e6f4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag3series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8d557-7898-42b7-8dc1-a8583e99180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag1order2series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbdb0a3-f6df-4c59-a57a-dae9d7204e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f9cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag1series = pd.Series(difference(water_predict1.water_demand[:168], interval=1, order=1))\n",
    "lag3series = pd.Series(difference(water_predict1.water_demand[:168], interval=3, order=1))\n",
    "lag1order2series = pd.Series(difference(\n",
    "    water_predict1.water_demand[:168], interval=1, order=2))\n",
    "\n",
    "fig = plt.figure(figsize=(14, 11))\n",
    "layout = (3, 2)\n",
    "original = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "lag1 = plt.subplot2grid(layout, (1, 0))\n",
    "lag3 = plt.subplot2grid(layout, (1, 1))\n",
    "lag1order2 = plt.subplot2grid(layout, (2, 0), colspan=2)\n",
    "\n",
    "original.set_title('Original series')\n",
    "original.plot(water_predict1.water_demand[:168], label='Original')\n",
    "original.plot(water_predict1.water_demand[:168].rolling(\n",
    "    12).mean(), color='red', label='Rolling Mean')\n",
    "original.plot(water_predict1.water_demand[:168].rolling(12).std(),\n",
    "              color='black', label='Rolling Std')\n",
    "original.legend(loc='best')\n",
    "\n",
    "lag1.set_title('Difference series with lag 1 order 1')\n",
    "lag1.plot(lag1series, label=\"Lag1\")\n",
    "lag1.plot(lag1series.rolling(7).mean(), color='red', label='Rolling Mean')\n",
    "lag1.plot(lag1series.rolling(7).std(), color='black', label='Rolling Std')\n",
    "lag1.legend(loc='best')\n",
    "\n",
    "lag3.set_title('Difference series with lag 3 order 1')\n",
    "lag3.plot(lag3series, label=\"Lag3\")\n",
    "lag3.plot(lag3series.rolling(7).mean(), color='red', label='Rolling Mean')\n",
    "lag3.plot(lag3series.rolling(7).std(), color='black', label='Rolling Std')\n",
    "lag3.legend(loc='best')\n",
    "\n",
    "lag1order2.set_title('Difference series with lag 1 order 2')\n",
    "lag1order2.plot(lag1order2series, label=\"Lag1order2\")\n",
    "lag1order2.plot(lag1order2series.rolling(7).mean(),\n",
    "                color='red', label='Rolling Mean')\n",
    "lag1order2.plot(lag1order2series.rolling(7).std(),\n",
    "                color='black', label='Rolling Std')\n",
    "lag1order2.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd1b76d",
   "metadata": {},
   "source": [
    "We can see how 1 order differencing doesnt really remove stationary but once we go with a order 2 difference it looks closer to a stationary series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a55c1",
   "metadata": {},
   "source": [
    "### Log scale transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d5f04",
   "metadata": {},
   "source": [
    "Applying a log scale transform to a time series could also  help remove the series dependence on time.\n",
    "\n",
    "This transform is done by substracting the previous obesvation to the current one.\n",
    "\n",
    "`LogScaleTransform(t)= Log(t)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0ed0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log = np.log(water_predict1.water_demand[:168])\n",
    "ts_log.plot(label='Log scale result')\n",
    "ts_log.rolling(window=12).mean().plot(color='red', label='Windowed mean')\n",
    "ts_log.rolling(window=12).std().plot(color='black', label='Std mean')\n",
    "plt.legend()\n",
    "plt.title('Log scale transformation into original series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be3618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COmment results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb58821a",
   "metadata": {},
   "source": [
    "### Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199dd1e5",
   "metadata": {},
   "source": [
    "We have seen the moving mean as a measure to check stationarity, we can also apply this windows to our series to remove seasonality.\n",
    "\n",
    "With smotthing we will take rolling averages over periods of time. Is a bit tricky to choose the best windows #MORE ON THIS IN NEXT SECTION WITH AUTO WINDOWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb4d568",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "avg = pd.Series(ts_log).rolling(12).mean()\n",
    "plt.plot(avg, label='Log scale smoothed with windows 12')\n",
    "avg.rolling(window=12).mean().plot(color='red', label='Windowed mean')\n",
    "avg.rolling(window=12).std().plot(color='black', label='Std mean')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf2241f",
   "metadata": {},
   "source": [
    "We can combine it with our previous log scale and apply differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_moving_avg_diff = ts_log - avg\n",
    "\n",
    "ts_log_moving_avg_diff.plot(label='Original')\n",
    "ts_log_moving_avg_diff.rolling(12).mean().plot(\n",
    "    color='red', label=\"Rolling mean\")\n",
    "ts_log_moving_avg_diff.rolling(12).std().plot(\n",
    "    color='black', label=\"Rolling mean\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5521df82",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false"
   },
   "source": [
    "# Methods for time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1fc69f",
   "metadata": {},
   "source": [
    "There are many methods that we can use for time series forecasting and there is not a clear winner. Model selection should always depend on how you data look and what are you trying to achieve. Some models may be more robust against outliers but perform worse than the more sensible and could still be the best choice depending on the use case.\n",
    "\n",
    "When looking at your data the main split is wether we have extra regressors (features) to our time series or just the series. Based on this we can start exploring different methods for forecasting and their performance in different metrics.\n",
    "\n",
    "In this section we will show models for both cases, time series with and without extra regressors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb816d",
   "metadata": {},
   "source": [
    "**Prepare data before modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc8736-a46d-4bcf-9e0a-7fda4a2af76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_predict[:168]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56cc3e-d9f0-44f4-84fc-2f96188e1484",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_predict[\"water_demand_last_month\"] = water_predict[\"water_demand\"]\n",
    "water_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc008b-e6c5-436e-8a22-723e4c553f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_predict[\"water_demand_last_month\"] = water_predict[\"water_demand_last_month\"].shift(periods=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c110983-93cc-47cd-86f4-52be519df256",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_predict.iloc[0,5] = 58554500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda9ced4-eaaa-4373-b097-36a971ddb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546f69c-f1c6-4f08-aef5-06c6dda3b2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f932ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split our dataset to be able to evaluate our models\n",
    "\n",
    "resultsDict = {}\n",
    "predictionsDict = {}\n",
    "\n",
    "split_date = '2019-01-01'\n",
    "df_training = water_predict[:168].loc[water_predict[:168].index <= split_date]\n",
    "df_test = water_predict[:168].loc[water_predict[:168].index > split_date]\n",
    "print(f\"{len(df_training)} days of training data \\n {len(df_test)} days of testing data \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ce013",
   "metadata": {},
   "source": [
    "It is also very important to include some naive forecast as the series mean or previous value to make sure our models perform better than the simplest of the simplest. We dont want to introduce any complexity if it does not provides any performance gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also add the naive mean average value\n",
    "mean = df_training.water_demand.mean()\n",
    "mean = np.array([mean for u in range(len(df_test))])\n",
    "resultsDict['Naive mean'] = evaluate(df_test.water_demand, mean)\n",
    "predictionsDict['Naive mean'] = mean\n",
    "resultsDict['Last Month value'] = evaluate(\n",
    "    df_test.water_demand, df_test.water_demand_last_month)\n",
    "predictionsDict['Last Month value'] = df_test.water_demand_last_month.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fbd4c9",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false"
   },
   "source": [
    "## Univariate-time-series-forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe0e98",
   "metadata": {},
   "source": [
    "In this section we will focus on time series forecasting methods capable of only looking at the target variable. This means no other regressors (more variables) can be added into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794bdd12",
   "metadata": {},
   "source": [
    "### Simple Exponential Smoothing (SES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e275a46f",
   "metadata": {},
   "source": [
    "The Simple Exponential Smoothing (SES) method models the next time step as an exponentially weighted linear function of observations at prior time steps. This method expects our time series to be non stationary in order to perform adecuately (no trend or seasonality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4415e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk throught the test data, training and predicting 1 day ahead for all the test data\n",
    "index = len(df_training)\n",
    "yhat = list()\n",
    "for t in tqdm(range(len(df_test.water_demand))):\n",
    "    temp_train = water_predict[:len(df_training)+t]\n",
    "    model = SimpleExpSmoothing(temp_train.water_demand)\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(start=len(temp_train), end=len(temp_train))\n",
    "    yhat = yhat + [predictions]\n",
    "\n",
    "yhat = pd.concat(yhat)\n",
    "resultsDict['SES'] = evaluate(df_test.water_demand, yhat.values)\n",
    "predictionsDict['SES'] = yhat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda50c2d-a45d-49a8-9b26-29a81039abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7b6274",
   "metadata": {},
   "source": [
    "### Holt Winterâ€™s Exponential Smoothing (HWES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c81a121",
   "metadata": {},
   "source": [
    "[HWES](https://machinelearningmastery.com/how-to-grid-search-triple-exponential-smoothing-for-time-series-forecasting-in-python/) or also known as triple exponential smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk throught the test data, training and predicting 1 day ahead for all the test data\n",
    "index = len(df_training)\n",
    "yhat = list()\n",
    "for t in tqdm(range(len(df_test.water_demand))):\n",
    "    temp_train = water_predict[:len(df_training)+t]\n",
    "    model = ExponentialSmoothing(temp_train.water_demand)\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(start=len(temp_train), end=len(temp_train))\n",
    "    yhat = yhat + [predictions]\n",
    "\n",
    "yhat = pd.concat(yhat)\n",
    "resultsDict['HWES'] = evaluate(df_test.water_demand, yhat.values)\n",
    "predictionsDict['HWES'] = yhat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54daf1b-632e-4c8e-a21c-fef47ab712fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d86e890",
   "metadata": {},
   "source": [
    "### Autoregression (AR)\n",
    "The autoregression (AR) method models the next step in the sequence as a linear function of the observations at prior time steps. Parameters of the model:\n",
    "\n",
    "- __Number of AR (Auto-Regressive) terms (p):__ p is the parameter associated with the auto-regressive aspect of the model, which incorporates past values i.e lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)â€¦.x(t-5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e21cc8-23a8-4ab1-a880-9a6b0c0c953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "#tatsmodels.tsa.ar_model.AutoReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681189ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk throught the test data, training and predicting 1 day ahead for all the test data\n",
    "index = len(df_training)\n",
    "yhat = list()\n",
    "for t in tqdm(range(len(df_test.water_demand))):\n",
    "    temp_train = water_predict[:len(df_training)+t]\n",
    "    model = AutoReg(temp_train.water_demand, lags=1)\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(\n",
    "        start=len(temp_train), end=len(temp_train), dynamic=False)\n",
    "    yhat = yhat + [predictions]\n",
    "\n",
    "yhat = pd.concat(yhat)\n",
    "resultsDict['AR'] = evaluate(df_test.water_demand, yhat.values)\n",
    "predictionsDict['AR'] = yhat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a41230-5b63-4d75-b577-5e1565a78ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955859f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_test.water_demand.values, label='Original')\n",
    "plt.plot(yhat.values, color='red', label='AR predicted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c9393a",
   "metadata": {},
   "source": [
    "### Moving Average (MA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2fb1f3",
   "metadata": {},
   "source": [
    "The Moving Average (MA) method models the next step in the sequence as the average of a window of observations at prior time steps. Parameters of the model:\n",
    "\n",
    "\n",
    "- __Number of MA (Moving Average) terms (q):__ q is size of the moving average part window of the model i.e. lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)â€¦.e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67184adf-4d23-4635-a8a1-23ac16e06364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA#, ARMA\n",
    "#statsmodels.tsa.arima.model.ARIMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a9f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MA example\n",
    "\n",
    "# Walk throught the test data, training and predicting 1 day ahead for all the test data\n",
    "index = len(df_training)\n",
    "yhat = list()\n",
    "for t in tqdm(range(len(df_test.water_demand))):\n",
    "    temp_train = water_predict[:len(df_training)+t]\n",
    "    model = ARIMA(temp_train.water_demand, order=(1, 0, 0))\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(\n",
    "        start=len(temp_train), end=len(temp_train), dynamic=False)\n",
    "    yhat = yhat + [predictions]\n",
    "\n",
    "yhat = pd.concat(yhat)\n",
    "resultsDict['MA'] = evaluate(df_test.water_demand, yhat.values)\n",
    "predictionsDict['MA'] = yhat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_test.water_demand.values, label='Original')\n",
    "plt.plot(yhat.values, color='red', label='MA predicted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3603db",
   "metadata": {},
   "source": [
    "### Autoregressive Moving Average (ARMA)\n",
    "\n",
    "This method will basically join the previous two `AR` and `MA`. Model parameters will be the sum of the two.\n",
    "\n",
    "- __Number of AR (Auto-Regressive) terms (p):__ p is the parameter associated with the auto-regressive aspect of the model, which incorporates past values i.e lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)â€¦.x(t-5).\n",
    "- __Number of MA (Moving Average) terms (q):__ q is size of the moving average part window of the model i.e. lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)â€¦.e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95092057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARMA example\n",
    "\n",
    "# Walk throught the test data, training and predicting 1 day ahead for all the test data\n",
    "index = len(df_training)\n",
    "yhat = list()\n",
    "for t in tqdm(range(len(df_test.water_demand))):\n",
    "    temp_train = water_predict[:len(df_training)+t]\n",
    "    model = ARIMA(temp_train.water_demand, order=(1, 1, 0))\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(\n",
    "        start=len(temp_train), end=len(temp_train), dynamic=False)\n",
    "    yhat = yhat + [predictions]\n",
    "\n",
    "yhat = pd.concat(yhat)\n",
    "resultsDict['ARMA'] = evaluate(df_test.water_demand, yhat.values)\n",
    "predictionsDict['ARMA'] = yhat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_test.water_demand.values, label='Original')\n",
    "plt.plot(yhat.values, color='red', label='ARMA predicted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116b303",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "### Autoregressive integrated moving average (ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188893f2",
   "metadata": {},
   "source": [
    "In an ARIMA model there are 3 parameters that are used to help model the major aspects of a times series: seasonality, trend, and noise. These parameters are labeled p,d,and q.\n",
    "\n",
    "* Number of AR (Auto-Regressive) terms (p): p is the parameter associated with the auto-regressive aspect of the model, which incorporates past values i.e lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)â€¦.x(t-5).\n",
    "* Number of Differences (d): d is the parameter associated with the integrated part of the model, which effects the amount of differencing to apply to a time series.\n",
    "* Number of MA (Moving Average) terms (q): q is size of the moving average part window of the model i.e. lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)â€¦.e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.\n",
    "\n",
    "**Tuning ARIMA parameters**\n",
    "\n",
    "Non stationarity series will require level of differencing (d) >0 in ARIMA\n",
    "Select the lag values for the Autoregression (AR) and Moving Average (MA) parameters, p and q respectively, using PACF, ACF plots\n",
    "AUTOARIMA\n",
    "\n",
    "Note: A problem with ARIMA is that it does not support seasonal data. That is a time series with a repeating cycle. ARIMA expects data that is either not seasonal or has the seasonal component removed, e.g. seasonally adjusted via methods such as seasonal differencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA example\n",
    "\n",
    "# Walk throught the test data, training and predicting 1 day ahead for all the test data\n",
    "index = len(df_training)\n",
    "yhat = list()\n",
    "for t in tqdm(range(len(df_test.water_demand))):\n",
    "    temp_train = water_predict[:len(df_training)+t]\n",
    "    model = ARIMA(temp_train.water_demand, order=(1, 0, 0))\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(\n",
    "        start=len(temp_train), end=len(temp_train), dynamic=False)\n",
    "    yhat = yhat + [predictions]\n",
    "\n",
    "yhat = pd.concat(yhat)\n",
    "resultsDict['ARIMA'] = evaluate(df_test.water_demand, yhat.values)\n",
    "predictionsDict['ARIMA'] = yhat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_test.water_demand.values, label='Original')\n",
    "plt.plot(yhat.values, color='red', label='ARIMA predicted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5574f7b",
   "metadata": {},
   "source": [
    "#### Auto ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "\n",
    "autoModel = pm.auto_arima(df_training.water_demand, trace=True,\n",
    "                          error_action='ignore', suppress_warnings=True, seasonal=False)\n",
    "autoModel.fit(df_training.water_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = autoModel.order\n",
    "yhat = list()\n",
    "for t in tqdm(range(len(df_test.water_demand))):\n",
    "    temp_train = air_pollution[:len(df_training)+t]\n",
    "    model = ARIMA(temp_train.water_demand, order=order)\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(\n",
    "        start=len(temp_train), end=len(temp_train), dynamic=False)\n",
    "    yhat = yhat + [predictions]\n",
    "\n",
    "yhat = pd.concat(yhat)\n",
    "resultsDict['AutoARIMA {0}'.format(order)] = evaluate(\n",
    "    df_test.water_demand, yhat)\n",
    "predictionsDict['AutoARIMA {0}'.format(order)] = yhat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ef771",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_test.water_demand.values, label='Original')\n",
    "plt.plot(yhat.values, color='red', label='AutoARIMA {0}'.format(order))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258094b",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "### Seasonal Autoregressive Integrated Moving-Average (SARIMA)\n",
    "Seasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component.\n",
    "\n",
    "It adds three new hyperparameters to specify the autoregression (AR), differencing (I) and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality.\n",
    "\n",
    "__Trend Elements:__\n",
    "\n",
    "There are three trend elements that require configuration. They are the same as the ARIMA model, specifically:\n",
    "\n",
    "- p: Trend autoregression order.\n",
    "- d: Trend difference order.\n",
    "- q: Trend moving average order.\n",
    "\n",
    "__Seasonal Elements:__\n",
    "\n",
    "There are four seasonal elements that are not part of ARIMA that must be configured; they are:\n",
    "\n",
    "- P: Seasonal autoregressive order.\n",
    "- D: Seasonal difference order.\n",
    "- Q: Seasonal moving average order.\n",
    "- m: The number of time steps for a single seasonal period. For example, an S of 12 for monthly data suggests a yearly seasonal cycle.\n",
    "\n",
    "__SARIMA notation:__\n",
    "SARIMA(p,d,q)(P,D,Q,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694644f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA example\n",
    "\n",
    "# Walk throught the test data, training and predicting 1 day ahead for all the test data\n",
    "index = len(df_training)\n",
    "yhat = list()\n",
    "for t in tqdm(range(len(df_test.water_demand))):\n",
    "    temp_train = water_predict[:len(df_training)+t]\n",
    "    model = SARIMAX(temp_train.water_demand, order=(\n",
    "        1, 0, 0), seasonal_order=(0, 0, 0, 3))\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(\n",
    "        start=len(temp_train), end=len(temp_train), dynamic=False)\n",
    "    yhat = yhat + [predictions]\n",
    "\n",
    "yhat = pd.concat(yhat)\n",
    "resultsDict['SARIMAX'] = evaluate(df_test.water_demand, yhat.values)\n",
    "predictionsDict['SARIMAX'] = yhat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ac4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_test.water_demand.values, label='Original')\n",
    "plt.plot(yhat.values, color='red', label='SARIMAX')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bbfbe8",
   "metadata": {},
   "source": [
    "#### Auto - SARIMA\n",
    "\n",
    "[auto_arima documentation for selecting best model](https://www.alkaline-ml.com/pmdarima/tips_and_tricks.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba79907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "autoModel = pm.auto_arima(df_training.water_demand, trace=True, error_action='ignore',\n",
    "                          suppress_warnings=True, seasonal=True, m=6, stepwise=True)\n",
    "autoModel.fit(df_training.water_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12022d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = autoModel.order\n",
    "seasonalOrder = autoModel.seasonal_order\n",
    "yhat = list()\n",
    "for t in tqdm(range(len(df_test.water_demand))):\n",
    "    temp_train = air_pollution[:len(df_training)+t]\n",
    "    model = SARIMAX(temp_train.water_demand, order=order,\n",
    "                    seasonal_order=seasonalOrder)\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(\n",
    "        start=len(temp_train), end=len(temp_train), dynamic=False)\n",
    "    yhat = yhat + [predictions]\n",
    "\n",
    "yhat = pd.concat(yhat)\n",
    "resultsDict['AutoSARIMAX {0},{1}'.format(order, seasonalOrder)] = evaluate(\n",
    "    df_test.water_demand, yhat.values)\n",
    "predictionsDict['AutoSARIMAX {0},{1}'.format(\n",
    "    order, seasonalOrder)] = yhat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c3ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_test.water_demand.values, label='Original')\n",
    "plt.plot(yhat.values, color='red', label='SARIMAX')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e7f66",
   "metadata": {},
   "source": [
    "### Prophet\n",
    "\n",
    "Prophet is a model released by [facebook](https://github.com/facebook/prophet). Is essentially a curve fitting approach, very similar in spirit to how BSTS models trend and seasonality, except that it uses generalized additive models instead of a state-space representation to describe each component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b27274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet needs some specifics data stuff, coment it here\n",
    "prophet_training = df_training.rename(\n",
    "    columns={'water_demand': 'y'})  # old method\n",
    "prophet_training['ds'] = prophet_training.index\n",
    "prophet_training.index = pd.RangeIndex(len(prophet_training.index))\n",
    "\n",
    "prophet_test = df_test.rename(columns={'water_demand': 'y'})  # old method\n",
    "prophet_test['ds'] = prophet_test.index\n",
    "prophet_test.index = pd.RangeIndex(len(prophet_test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc74828",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet = Prophet(\n",
    "    growth='linear',\n",
    "    seasonality_mode='multiplicative',\n",
    "    yearly_seasonality=False\n",
    ").add_seasonality(\n",
    "    name='monthly',\n",
    "    period=30.5,\n",
    "    fourier_order=55\n",
    ").add_seasonality(\n",
    "    name='yearly',\n",
    "    period=365.25,\n",
    "    fourier_order=20\n",
    ").add_seasonality(\n",
    "    name='quarterly',\n",
    "    period=365.25/4,\n",
    "    fourier_order=55\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d759d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet.fit(prophet_training)\n",
    "yhat = prophet.predict(prophet_test)\n",
    "resultsDict['Prophet univariate'] = evaluate(\n",
    "    df_test.water_demand, yhat.yhat.values)\n",
    "predictionsDict['Prophet univariate'] = yhat.yhat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe528ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_test.water_demand.values, label='Original')\n",
    "plt.plot(yhat.yhat, color='red', label='Prophet univariate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667bfd6c",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false"
   },
   "source": [
    "## Multivariate time series forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD time features to our model\n",
    "def create_time_features(df, target=None):\n",
    "    \"\"\"\n",
    "    Creates time series features from datetime index\n",
    "    \"\"\"\n",
    "    df['date'] = df.index\n",
    "    #df['hour'] = df['date'].dt.hour\n",
    "    #df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    #df['quarter'] = df['date'].dt.quarter\n",
    "    #df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    #df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    #df['sin_day'] = np.sin(df['dayofyear'])\n",
    "    #df['cos_day'] = np.cos(df['dayofyear'])\n",
    "    #df['dayofmonth'] = df['date'].dt.day\n",
    "    #df['weekofyear'] = df['date'].dt.weekofyear\n",
    "    X = df.drop(['date'], axis=1)\n",
    "    if target:\n",
    "        y = df[target]\n",
    "        X = X.drop([target], axis=1)\n",
    "        return X, y\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72020e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df, y_train = create_time_features(\n",
    "    df_training, target='water_demand')\n",
    "X_test_df, y_test = create_time_features(df_test, target='water_demand')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_df)  # No cheating, never scale on the training+test!\n",
    "X_train = scaler.transform(X_train_df)\n",
    "X_test = scaler.transform(X_test_df)\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train, columns=X_train_df.columns)\n",
    "X_test_df = pd.DataFrame(X_test, columns=X_test_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f762d7e",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false"
   },
   "source": [
    "### Linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646cc76f",
   "metadata": {},
   "source": [
    "#### Bayesian regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45813e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.BayesianRidge()\n",
    "reg.fit(X_train, y_train)\n",
    "yhat = reg.predict(X_test)\n",
    "resultsDict['BayesianRidge'] = evaluate(df_test.pollution_today, yhat)\n",
    "predictionsDict['BayesianRidge'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452edb5",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be05a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "reg.fit(X_train, y_train)\n",
    "yhat = reg.predict(X_test)\n",
    "resultsDict['Lasso'] = evaluate(df_test.water_demand, yhat)\n",
    "predictionsDict['Lasso'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c3b72",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "### Tree models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe40c09",
   "metadata": {},
   "source": [
    "#### Randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f778029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "reg.fit(X_train, y_train)\n",
    "yhat = reg.predict(X_test)\n",
    "resultsDict['Randomforest'] = evaluate(df_test.water_demand, yhat)\n",
    "predictionsDict['Randomforest'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef01342",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11379748",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n",
    "reg.fit(X_train, y_train,\n",
    "        verbose=False)  # Change verbose to True if you want to see it train\n",
    "yhat = reg.predict(X_test)\n",
    "resultsDict['XGBoost'] = evaluate(df_test.water_demand, yhat)\n",
    "predictionsDict['XGBoost'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f57a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_test.water_demand.values, label='Original')\n",
    "plt.plot(yhat, color='red', label='XGboost')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47959fd7",
   "metadata": {},
   "source": [
    "#### Lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad86b2",
   "metadata": {},
   "source": [
    "A tree gradient boosting model by [microsoft](https://github.com/microsoft/LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eac8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightGBM = lgb.LGBMRegressor()\n",
    "lightGBM.fit(X_train, y_train)\n",
    "yhat = lightGBM.predict(X_test)\n",
    "resultsDict['Lightgbm'] = evaluate(df_test.water_demand, yhat)\n",
    "predictionsDict['Lightgbm'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f83857",
   "metadata": {},
   "source": [
    "### Support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76090e9d",
   "metadata": {},
   "source": [
    "Explain multiple kernels balbla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07bd58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = svm.SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "reg.fit(X_train, y_train)\n",
    "yhat = reg.predict(X_test)\n",
    "resultsDict['SVM RBF'] = evaluate(df_test.water_demand, yhat)\n",
    "predictionsDict['SVM RBF'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2fb6e8",
   "metadata": {},
   "source": [
    "### Nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf2dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = KNeighborsRegressor(n_neighbors=2)\n",
    "reg.fit(X_train, y_train)\n",
    "yhat = reg.predict(X_test)\n",
    "resultsDict['Kneighbors'] = evaluate(df_test.water_demand, yhat)\n",
    "predictionsDict['Kneighbors'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8505491e",
   "metadata": {},
   "source": [
    "### Prophet multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet = Prophet(\n",
    "    growth='linear',\n",
    "    seasonality_mode='multiplicative',\n",
    "    #daily_seasonality=True,\n",
    ")#.add_country_holidays(country_name='China')\n",
    "\n",
    "\n",
    "for col in prophet_training.columns:\n",
    "    if col not in [\"ds\", \"y\"]:\n",
    "        prophet.add_regressor(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2877488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet.fit(prophet_training)\n",
    "yhat = prophet.predict(prophet_test)\n",
    "resultsDict['Prophet multivariate'] = evaluate(y_test, yhat.yhat.values)\n",
    "predictionsDict['Prophet multivariate'] = yhat.yhat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ca223",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_test.water_demand.values, label='Original')\n",
    "plt.plot(yhat.yhat, color='red', label='Prophet multivariate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3749cf",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false"
   },
   "source": [
    "### Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80418e38",
   "metadata": {},
   "source": [
    "#### Tensorlfow LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3fd722",
   "metadata": {},
   "source": [
    "LSTM are a special type of neural network architecture, you can read more on this [here](https://www.tensorflow.org/guide/keras/rnn)\n",
    "\n",
    "We will be trying a LSTM model for our benchmark but we will need to reshape our data to provide the network a window of previous samples (past days data) for each y target value. Find the code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our dl model we will create windows of data that will be feeded into the datasets, for each timestemp T we will append the data from T-7 to T to the Xdata with target Y(t)\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 100\n",
    "WINDOW_LENGTH = 24\n",
    "\n",
    "\n",
    "def window_data(X, Y, window=7):\n",
    "    '''\n",
    "    The dataset length will be reduced to guarante all samples have the window, so new length will be len(dataset)-window\n",
    "    '''\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(window-1, len(X)):\n",
    "        x.append(X[i-window+1:i+1])\n",
    "        y.append(Y[i])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "\n",
    "# Since we are doing sliding, we need to join the datasets again of train and test\n",
    "X_w = np.concatenate((X_train, X_test))\n",
    "y_w = np.concatenate((y_train, y_test))\n",
    "\n",
    "X_w, y_w = window_data(X_w, y_w, window=WINDOW_LENGTH)\n",
    "X_train_w = X_w[:-len(X_test)]\n",
    "y_train_w = y_w[:-len(X_test)]\n",
    "X_test_w = X_w[-len(X_test):]\n",
    "y_test_w = y_w[-len(X_test):]\n",
    "\n",
    "# Check we will have same test set as in the previous models, make sure we didnt screw up on the windowing\n",
    "print(f\"Test set equal: {np.array_equal(y_test_w,y_test)}\")\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train_w, y_train_w))\n",
    "train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data = tf.data.Dataset.from_tensor_slices((X_test_w, y_test_w))\n",
    "val_data = val_data.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8d68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.0\n",
    "simple_lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(\n",
    "        128, input_shape=X_train_w.shape[-2:], dropout=dropout),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "simple_lstm_model.compile(optimizer='rmsprop', loss='mae')\n",
    "\n",
    "# logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") #Support for tensorboard tracking!\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd98ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_INTERVAL = 200\n",
    "EPOCHS = 5\n",
    "\n",
    "model_history = simple_lstm_model.fit(train_data, epochs=EPOCHS,\n",
    "                                      steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                      validation_data=val_data, validation_steps=50)  # ,callbacks=[tensorboard_callback]) #Uncomment this line for tensorboard support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde90797",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = simple_lstm_model.predict(X_test_w).reshape(1, -1)[0]\n",
    "resultsDict['Tensorflow simple LSTM'] = evaluate(y_test, yhat)\n",
    "predictionsDict['Tensorflow simple LSTM'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f135af1b",
   "metadata": {},
   "source": [
    "#### DeepAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0752018",
   "metadata": {},
   "source": [
    "[DeepAR](https://arxiv.org/pdf/1704.04110.pdf) is a deep learning architecture released by amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ca92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['dew', 'temp', 'press', 'wnd_spd', 'snow', 'rain',\n",
    "            'pollution_yesterday', 'hour', 'dayofweek', 'quarter', 'month',\n",
    "            'year', 'dayofyear', 'sin_day', 'cos_day', 'dayofmonth', 'weekofyear']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # No cheating, never scale on the training+test!\n",
    "df_training[features] = scaler.transform(df_training[features])\n",
    "df_test[features] = scaler.transform(df_test[features])\n",
    "\n",
    "\n",
    "training_data = ListDataset(\n",
    "    [{\"start\": df_training.index[0], \"target\": df_training.pollution_today,\n",
    "      'feat_dynamic_real': [df_training[feature] for feature in features]\n",
    "      }],\n",
    "    freq=\"d\"\n",
    ")\n",
    "test_data = ListDataset(\n",
    "    [{\"start\": df_test.index[0], \"target\": df_test.pollution_today,\n",
    "      'feat_dynamic_real': [df_test[feature] for feature in features]\n",
    "      }],\n",
    "    freq=\"d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad068a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DeepAREstimator(freq=\"d\",\n",
    "                            prediction_length=1, context_length=30,\n",
    "                            trainer=Trainer(epochs=5))\n",
    "\n",
    "predictor = estimator.train(training_data=training_data)\n",
    "\n",
    "\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    test_data, predictor=predictor, num_samples=len(df_test))\n",
    "\n",
    "forecasts = list(forecast_it)\n",
    "tss = list(ts_it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = forecasts[0].samples.reshape(1, -1)[0]\n",
    "resultsDict['DeepAR'] = evaluate(y_test, yhat)\n",
    "predictionsDict['DeepAR'] = yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384c7ff",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false"
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b82a8",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcde921",
   "metadata": {},
   "source": [
    "We have seen models with really low amount of parameters (Auto regression models,Linear models) or with crazy ammount (Trees,Prophet). Some models are more robust to different data types/shapes and dont need any hyperparameter optimization but some other can give you poor results if the parameters are not tunned, we can tune the model parameters to better fit our dataset properties. We can do this manually with pure knowledge about the model but this becames really hard when the model contains a lot of different parameters, this is when hyperparameter optimization comes handy.\n",
    "\n",
    "Hyperparameter optimization is trying to find the best parameters in an automatic way. We present two methods that are used frequently:\n",
    "\n",
    "* **Grid search** Brute force method to try all different possible combinations of parameters. Will always find the best combination\n",
    "* **Bayesian processes** \"Brute\" force method, optimizes parameter search by using gausian processes to model each parameter distribution and don't go over all the possible values. Really nice library for python https://github.com/fmfn/BayesianOptimization, this method will not always find the best combination of parameters\n",
    "\n",
    "We provide 1 example for each method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca5821",
   "metadata": {},
   "source": [
    "### Grid search - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d373341",
   "metadata": {},
   "source": [
    "With grid search we can use the handy sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GridSearchCV(svm.SVR(kernel='rbf', gamma=0.1),\n",
    "                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n",
    "                               \"gamma\": np.logspace(-2, 2, 5)})\n",
    "reg.fit(X_train, y_train)\n",
    "yhat = reg.predict(X_test)\n",
    "resultsDict['SVM RBF GRID SEARCH'] = evaluate(df_test.pollution_today, yhat)\n",
    "predictionsDict['SVM RBF GRID SEARCH'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dcdb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "increase = 1 - (resultsDict['SVM RBF GRID SEARCH']\n",
    "                ['rmse']/resultsDict['SVM RBF']['rmse'])\n",
    "print(\n",
    "    f\"Grid search Tunned SVM is {increase*100}% better than the SVM with default parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8232812c",
   "metadata": {},
   "source": [
    "### Bayesian processes - Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3869e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(y_actual, y_predicted):\n",
    "    return sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n",
    "\n",
    "my_scorer = make_scorer(rms, greater_is_better=False)\n",
    "pbounds = {\n",
    "    'n_estimators': (100, 10000),\n",
    "    'max_depth': (3, 15),\n",
    "    'min_samples_leaf': (1, 4),\n",
    "    'min_samples_split': (2, 10),\n",
    "}\n",
    "\n",
    "\n",
    "def rf_hyper_param(n_estimators,\n",
    "                   max_depth,\n",
    "                   min_samples_leaf,\n",
    "                   min_samples_split):\n",
    "\n",
    "    max_depth = int(max_depth)\n",
    "    n_estimators = int(n_estimators)\n",
    "\n",
    "    clf = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                                max_depth=int(max_depth),\n",
    "                                min_samples_leaf=int(min_samples_leaf),\n",
    "                                min_samples_split=int(min_samples_split),\n",
    "                                n_jobs=1)\n",
    "\n",
    "    return -np.mean(cross_val_score(clf, X_train, y_train, cv=3))\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=rf_hyper_param,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671665e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.maximize(\n",
    "    init_points=3,\n",
    "    n_iter=20,\n",
    "    acq='ei'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa92f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = optimizer.max['params']\n",
    "\n",
    "# Converting the max_depth and n_estimator values from float to int\n",
    "params['max_depth'] = int(params['max_depth'])\n",
    "params['n_estimators'] = int(params['n_estimators'])\n",
    "params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "params['min_samples_split'] = int(params['min_samples_split'])\n",
    "\n",
    "# Initialize an XGBRegressor with the tuned parameters and fit the training data\n",
    "tunned_rf = RandomForestRegressor(**params)\n",
    "# Change verbose to True if you want to see it train\n",
    "tunned_rf.fit(X_train, y_train)\n",
    "\n",
    "yhat = tunned_rf.predict(X_test)\n",
    "resultsDict['Randomforest tunned'] = evaluate(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf58f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "increase = 1 - (resultsDict['Randomforest tunned']\n",
    "                ['rmse']/resultsDict['Randomforest']['rmse'])\n",
    "print(\n",
    "    f\"Bayesian optimized Randomforest is {increase*100}% better than the Randomforest with default parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcce002",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c6582",
   "metadata": {},
   "source": [
    "Ensembling refers to combine multiple models to achieve a better performance, most of the time this only makes sense when models have similar performance but predict values differently so we try to get the best of each model.\n",
    "\n",
    "We will pick our 3 top performing models and look at the correlation of their residuals, the less correlated the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b841c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Tensorflow simple LSTM',\n",
    "          'Lightgbm',\n",
    "          'XGBoost']\n",
    "resis = pd.DataFrame(data={k: df_test.pollution_today.values -\n",
    "                     v for k, v in predictionsDict.items()})[models]\n",
    "corr = resis.corr()\n",
    "print(\"Residuals correlation\")\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2368cb",
   "metadata": {},
   "source": [
    "We can see how both tree models are a bit similar ~0.87 but quite different from the Deep Learning model with corr ~0.7. In this case it would really make sense to ensemble the methods and see how they behave. The most reasonable combinations to try would be\n",
    "\n",
    "* XGboost + Tensorflow\n",
    "* XGBoost + Lightgbm\n",
    "* Lightgbm + Tensorflow\n",
    "* XGBoost + Lightgbm + Tensorflow\n",
    "\n",
    "We will just sum the predictions of each model with similar weights (0.5 if two models, 0.333 if three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dfd074",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsDict['EnsembleXG+LIGHT'] = (\n",
    "    predictionsDict['XGBoost'] + predictionsDict['Lightgbm'])/2\n",
    "resultsDict['EnsembleXG+LIGHT'] = evaluate(\n",
    "    df_test.pollution_today.values, predictionsDict['EnsembleXG+LIGHT'])\n",
    "\n",
    "predictionsDict['EnsembleXG+LIGHT+TF'] = (predictionsDict['XGBoost'] +\n",
    "                                          predictionsDict['Lightgbm'] + predictionsDict['Tensorflow simple LSTM'])/3\n",
    "resultsDict['EnsembleXG+LIGHT+TF'] = evaluate(\n",
    "    df_test.pollution_today.values, predictionsDict['EnsembleXG+LIGHT+TF'])\n",
    "\n",
    "predictionsDict['EnsembleLIGHT+TF'] = (\n",
    "    predictionsDict['Lightgbm'] + predictionsDict['Tensorflow simple LSTM'])/2\n",
    "resultsDict['EnsembleLIGHT+TF'] = evaluate(\n",
    "    df_test.pollution_today.values, predictionsDict['EnsembleLIGHT+TF'])\n",
    "\n",
    "predictionsDict['EnsembleXG+TF'] = (predictionsDict['XGBoost'] +\n",
    "                                    predictionsDict['Tensorflow simple LSTM'])/2\n",
    "resultsDict['EnsembleXG+TF'] = evaluate(\n",
    "    df_test.pollution_today.values, predictionsDict['EnsembleXG+TF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd9984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_extent(ax, pad=0.0):\n",
    "    \"\"\"Get the full extent of an axes, including axes labels, tick labels, and\n",
    "    titles.\"\"\"\n",
    "    # For text objects, we need to draw the figure first, otherwise the extents\n",
    "    # are undefined.\n",
    "    ax.figure.canvas.draw()\n",
    "    items = ax.get_xticklabels() + ax.get_yticklabels()\n",
    "#    items += [ax, ax.title, ax.xaxis.label, ax.yaxis.label]\n",
    "    items += [ax, ax.title]\n",
    "    bbox = Bbox.union([item.get_window_extent() for item in items])\n",
    "\n",
    "    return bbox.expanded(1.0 + pad, 1.0 + pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_metrics(resultsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd66d0",
   "metadata": {},
   "source": [
    "Looks like we get even better performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687aaea",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25e7f7a",
   "metadata": {},
   "source": [
    "Some models allow for for native feature importance algorithms but I personally like the library [SHAP](https://github.com/slundberg/shap) that provides a game theory approach to measure how each feature affects our forecast.\n",
    "\n",
    "Here is an example on how to use SHAP for our Lightgbm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d0a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(lightGBM)\n",
    "shap_values = explainer.shap_values(X_train_df)\n",
    "shap.summary_plot(shap_values, X_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(resultsDict).transpose().iloc[::-1]\n",
    "df = df.round(2)\n",
    "df.to_csv(\"results/results_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b3043",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_html(\"results/results_summary.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd4edf",
   "metadata": {},
   "source": [
    "# Possible improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b1bff3",
   "metadata": {},
   "source": [
    "* Parameter tunned lightgbm and xgboost and redo the ensemble with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55801d0",
   "metadata": {},
   "source": [
    "\n",
    "# Additional resources and literature\n",
    "| | |\n",
    "| - | - |\n",
    "| Adhikari, R., & Agrawal, R. K. (2013). An introductory study on time series modeling and forecasting | [[1]](https://arxiv.org/ftp/arxiv/papers/1302/1302.6613.pdf)|\n",
    "| Introduction to Time Series Forecasting With Python | [[2]](https://machinelearningmastery.com/introduction-to-time-series-forecasting-with-python/)|\n",
    "| Deep Learning for Time Series Forecasting | [[3]](https://machinelearningmastery.com/deep-learning-for-time-series-forecasting/ )\n",
    "| The Complete Guide to Time Series Analysis and Forecasting| [[4]](https://towardsdatascience.com/the-complete-guide-to-time-series-analysis-and-forecasting-70d476bfe775)|\n",
    "| How to Decompose Time Series Data into Trend and Seasonality| [[5]](https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
